{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNrGHbtPl8bFq0oLs5oWee5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amatyaagrata/ConceptandTechnologyofAI/blob/main/Workshop8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Step -1- Building a Custom Decision Tree with Information Gain:"
      ],
      "metadata": {
        "id": "1GHsLf8l9ZXx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "mkeUE6eo5r9V"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "class CustomDecisionTree:\n",
        "    def __init__(self, max_depth=None):\n",
        "        \"\"\"\n",
        "        Initializes the decision tree.\n",
        "        Parameters:\n",
        "        max_depth (int, optional): Maximum depth of the tree.\n",
        "        \"\"\"\n",
        "        self.max_depth = max_depth\n",
        "        self.tree = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"\n",
        "        Trains the decision tree model.\n",
        "        \"\"\"\n",
        "        self.tree = self._build_tree(X, y, depth=0)\n",
        "\n",
        "    def _build_tree(self, X, y, depth):\n",
        "        \"\"\"\n",
        "        Recursively builds the decision tree.\n",
        "        \"\"\"\n",
        "        num_samples, num_features = X.shape\n",
        "        unique_classes = np.unique(y)\n",
        "\n",
        "        # Stopping conditions\n",
        "        if len(unique_classes) == 1:\n",
        "            return {'class': unique_classes[0]}\n",
        "\n",
        "        if num_samples == 0 or (self.max_depth is not None and depth >= self.max_depth):\n",
        "            return {'class': np.bincount(y).argmax()}\n",
        "\n",
        "        best_info_gain = -np.inf\n",
        "        best_split = None\n",
        "\n",
        "        for feature_idx in range(num_features):\n",
        "            thresholds = np.unique(X[:, feature_idx])\n",
        "            for threshold in thresholds:\n",
        "                left_mask = X[:, feature_idx] <= threshold\n",
        "                right_mask = X[:, feature_idx] > threshold\n",
        "\n",
        "                left_y = y[left_mask]\n",
        "                right_y = y[right_mask]\n",
        "\n",
        "                if len(left_y) == 0 or len(right_y) == 0:\n",
        "                    continue\n",
        "\n",
        "                info_gain = self._information_gain(y, left_y, right_y)\n",
        "\n",
        "                if info_gain > best_info_gain:\n",
        "                    best_info_gain = info_gain\n",
        "                    best_split = {\n",
        "                        'feature_idx': feature_idx,\n",
        "                        'threshold': threshold,\n",
        "                        'left_mask': left_mask,\n",
        "                        'right_mask': right_mask\n",
        "                    }\n",
        "\n",
        "        if best_split is None:\n",
        "            return {'class': np.bincount(y).argmax()}\n",
        "\n",
        "        left_tree = self._build_tree(\n",
        "            X[best_split['left_mask']],\n",
        "            y[best_split['left_mask']],\n",
        "            depth + 1\n",
        "        )\n",
        "\n",
        "        right_tree = self._build_tree(\n",
        "            X[best_split['right_mask']],\n",
        "            y[best_split['right_mask']],\n",
        "            depth + 1\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'feature_idx': best_split['feature_idx'],\n",
        "            'threshold': best_split['threshold'],\n",
        "            'left_tree': left_tree,\n",
        "            'right_tree': right_tree\n",
        "        }\n",
        "\n",
        "    def _information_gain(self, parent, left, right):\n",
        "        \"\"\"\n",
        "        Computes Information Gain.\n",
        "        \"\"\"\n",
        "        parent_entropy = self._entropy(parent)\n",
        "        left_entropy = self._entropy(left)\n",
        "        right_entropy = self._entropy(right)\n",
        "\n",
        "        weighted_entropy = (\n",
        "            (len(left) / len(parent)) * left_entropy +\n",
        "            (len(right) / len(parent)) * right_entropy\n",
        "        )\n",
        "\n",
        "        return parent_entropy - weighted_entropy\n",
        "\n",
        "    def _entropy(self, y):\n",
        "        \"\"\"\n",
        "        Computes entropy.\n",
        "        \"\"\"\n",
        "        class_probs = np.bincount(y) / len(y)\n",
        "        return -np.sum(class_probs * np.log2(class_probs + 1e-9))\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Predicts class labels for X.\n",
        "        \"\"\"\n",
        "        return np.array([self._predict_single(x, self.tree) for x in X])\n",
        "\n",
        "    def _predict_single(self, x, tree):\n",
        "        \"\"\"\n",
        "        Predicts a single sample.\n",
        "        \"\"\"\n",
        "        if 'class' in tree:\n",
        "            return tree['class']\n",
        "\n",
        "        if x[tree['feature_idx']] <= tree['threshold']:\n",
        "            return self._predict_single(x, tree['left_tree'])\n",
        "        else:\n",
        "            return self._predict_single(x, tree['right_tree'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step -2- Load and Split the Iris Datasets:"
      ],
      "metadata": {
        "id": "muj7aOpQ9azH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Necessary Imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "# Load the Iris dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split into training and test sets (80% training, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "DMz0Bao76IIM"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step -3- Train and Evaluate a Custom Decision Tree:"
      ],
      "metadata": {
        "id": "I_FzuZsd9fVR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Train the custom decision tree\n",
        "custom_tree = CustomDecisionTree(max_depth=3)\n",
        "custom_tree.fit(X_train, y_train)\n",
        "# Predict on the test set\n",
        "y_pred_custom = custom_tree.predict(X_test)\n",
        "# Calculate accuracy\n",
        "accuracy_custom = accuracy_score(y_test, y_pred_custom)\n",
        "print(f\"Custom Decision Tree Accuracy: {accuracy_custom:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZfTpF2vk6dy0",
        "outputId": "3b7c7b63-88b7-4794-8ff2-afce079bab2b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Custom Decision Tree Accuracy: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step -4- Train and Evaluate a Scikit Learn Decision Tree:"
      ],
      "metadata": {
        "id": "PcOOhBiI9iXB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the Scikit-learn decision tree\n",
        "sklearn_tree = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
        "sklearn_tree.fit(X_train, y_train)\n",
        "# Predict on the test set\n",
        "y_pred_sklearn = sklearn_tree.predict(X_test)\n",
        "# Calculate accuracy\n",
        "accuracy_sklearn = accuracy_score(y_test, y_pred_sklearn)\n",
        "print(f\"Scikit-learn Decision Tree Accuracy: {accuracy_sklearn:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WeK8clAk6lU5",
        "outputId": "3dce8447-4bb7-4c11-892c-5784a2835965"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scikit-learn Decision Tree Accuracy: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step -5- Result Comparison:"
      ],
      "metadata": {
        "id": "SbhMrNUq9k--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Accuracy Comparison:\")\n",
        "print(f\"Custom Decision Tree: {accuracy_custom:.4f}\")\n",
        "print(f\"Scikit-learn Decision Tree: {accuracy_sklearn:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nnCZf7Py6oa1",
        "outputId": "4e65b549-b545-43da-b264-05ec77b40aab"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy Comparison:\n",
            "Custom Decision Tree: 1.0000\n",
            "Scikit-learn Decision Tree: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Exercise - Ensemble Methods and Hyperparameter Tuning.\n",
        "Using the Wine Dataset from scikit-learn\n",
        "1. Implement Classification Models:\n",
        "• Train a Decision Tree Classifier and a Random Forest Classifier using scikit-learn.\n",
        "• Compare the models based on their F1 scores."
      ],
      "metadata": {
        "id": "ZWrlTZoAwvvq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "wine = load_wine()\n",
        "X = wine.data\n",
        "y = wine.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Decision Tree\n",
        "dt = DecisionTreeClassifier(random_state=42)\n",
        "dt.fit(X_train, y_train)\n",
        "dt_preds = dt.predict(X_test)\n",
        "print(\"Decision Tree F1:\", f1_score(y_test, dt_preds, average='weighted'))\n",
        "\n",
        "# Random Forest\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "rf_preds = rf.predict(X_test)\n",
        "print(\"Random Forest F1:\", f1_score(y_test, rf_preds, average='weighted'))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bSoontFvwwo7",
        "outputId": "6be3521e-4eb9-49fb-e236-31c7e8e46428"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree F1: 0.9439974457215836\n",
            "Random Forest F1: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2. Hyperparameter Tuning:\n",
        "• Identify three hyperparameters of the Random Forest Classifier.\n",
        "• Perform hyperparameter tuning using GridSearchCV to optimize these parameters.\n",
        "• Take hints from the scikit-learn documentation to guide the implementation."
      ],
      "metadata": {
        "id": "WkKbvgs1xA-g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100],\n",
        "    'max_depth': [None, 5],\n",
        "    'min_samples_split': [2, 5]\n",
        "}\n",
        "\n",
        "grid = GridSearchCV(RandomForestClassifier(random_state=42),\n",
        "                    param_grid, scoring='f1_weighted', cv=3)\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best Parameters:\", grid.best_params_)\n",
        "best_rf = grid.best_estimator_\n",
        "print(\"F1 with Best RF:\", f1_score(y_test, best_rf.predict(X_test), average='weighted'))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DJurzyJxxCMI",
        "outputId": "a46444fe-2ccc-48f7-daf1-cf0455c37c6e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'max_depth': None, 'min_samples_split': 2, 'n_estimators': 100}\n",
            "F1 with Best RF: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3. Implement Regression Model:\n",
        "• Train a Decision Tree Regressor and a Random Forest Regressor using scikit-learn.\n",
        "• Identify three parameters for Random Forest Regressio and Perform hyperparameter tuning using\n",
        "RandomSearchCV to optimize these parameters."
      ],
      "metadata": {
        "id": "tgIcAD3hxKQB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "X_reg = wine.data[:, 1:]\n",
        "y_reg = wine.data[:, 0]\n",
        "\n",
        "X_train_r, X_test_r, y_train_r, y_test_r = train_test_split(X_reg, y_reg, test_size=0.2, random_state=42)\n",
        "\n",
        "# Decision Tree Regressor\n",
        "dt_r = DecisionTreeRegressor(random_state=42)\n",
        "dt_r.fit(X_train_r, y_train_r)\n",
        "dt_preds = dt_r.predict(X_test_r)\n",
        "print(\"Decision Tree MSE:\", mean_squared_error(y_test_r, dt_preds))\n",
        "\n",
        "# Random Forest Regressor\n",
        "rf_r = RandomForestRegressor(random_state=42)\n",
        "rf_r.fit(X_train_r, y_train_r)\n",
        "rf_preds = rf_r.predict(X_test_r)\n",
        "print(\"Random Forest MSE:\", mean_squared_error(y_test_r, rf_preds))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4xzZVc3YxLCe",
        "outputId": "93ce066c-2a80-403b-a0c1-defa762b8b3f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree MSE: 0.31197222222222226\n",
            "Random Forest MSE: 0.15426672999999946\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "params = {\n",
        "    'n_estimators': [50, 100],\n",
        "    'max_depth': [None, 5],\n",
        "    'min_samples_leaf': [1, 2]\n",
        "}\n",
        "\n",
        "rand_search = RandomizedSearchCV(RandomForestRegressor(random_state=42),\n",
        "                                 param_distributions=params, n_iter=4, cv=3)\n",
        "rand_search.fit(X_train_r, y_train_r)\n",
        "\n",
        "best_rf_r = rand_search.best_estimator_\n",
        "print(\"Best Parameters:\", rand_search.best_params_)\n",
        "print(\"MSE with Best RF:\", mean_squared_error(y_test_r, best_rf_r.predict(X_test_r)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ae02PxJQxOJ9",
        "outputId": "577367ea-fdec-4d15-b9c5-f7d891258b11"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'n_estimators': 50, 'min_samples_leaf': 2, 'max_depth': 5}\n",
            "MSE with Best RF: 0.13569595862301947\n"
          ]
        }
      ]
    }
  ]
}